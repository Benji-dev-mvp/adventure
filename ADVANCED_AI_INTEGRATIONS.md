# Advanced AI Integrations - Complete Implementation

## ðŸŽ¯ Overview

This document details the complete integration of 5 cutting-edge AI frameworks into the Artisan B2B sales automation platform. These integrations provide production-ready, enterprise-grade AI capabilities that enhance every aspect of the sales process.

## ðŸ“¦ Integrated Frameworks

### 1. **LangChain** (67kâ­ GitHub)
- **Purpose**: Agent orchestration, tool management, and memory
- **Repository**: `langchain-ai/langchain`
- **Integration**: `backend/app/integrations/langchain_agent.py`
- **Key Features**:
  - ReAct (Reasoning + Action) agent pattern
  - Multi-tool execution and chaining
  - Conversation memory (buffer + vector store)
  - Support for OpenAI and Anthropic models
  - Custom tools for B2B sales automation

### 2. **Pydantic AI** (14kâ­ GitHub)
- **Purpose**: Type-safe AI agents with structured outputs
- **Repository**: `pydantic/pydantic-ai`
- **Integration**: `backend/app/integrations/pydantic_agent.py`
- **Key Features**:
  - Full type inference for tools and outputs
  - Automatic validation with retry logic
  - Dependency injection via RunContext
  - Structured output validation using Pydantic models
  - Integration with OpenAI and Anthropic

### 3. **Mem0** (24kâ­ GitHub)
- **Purpose**: Advanced persistent memory layer for AI agents
- **Repository**: `mem0ai/mem0`
- **Integration**: `backend/app/integrations/mem0_memory.py`
- **Key Features**:
  - User-partitioned memory (by user_id, agent_id, app_id)
  - Semantic search with vector similarity
  - Cross-session memory retention
  - Redis + Qdrant backing for persistence
  - Memory importance scoring and decay

### 4. **LlamaIndex** (40kâ­ GitHub)
- **Purpose**: RAG (Retrieval-Augmented Generation) and document ingestion
- **Repository**: `run-llama/llama_index`
- **Integration**: `backend/app/integrations/llamaindex_rag.py`
- **Key Features**:
  - 100+ data connectors (APIs, PDFs, SQL, Google Drive, Notion)
  - Vector indexing with semantic search
  - Multi-query retrieval with context augmentation
  - Chat engines with conversation memory
  - Metadata filtering and hybrid search

### 5. **Reflex** (22kâ­ GitHub)
- **Purpose**: Python-only fullstack framework (optional alternative to React)
- **Repository**: `reflex-dev/reflex`
- **Status**: Installed but not integrated (optional for future UI migration)
- **Use Case**: If team wants to eliminate JavaScript and build UI entirely in Python

---

## ðŸ—ï¸ Architecture

### Unified Orchestration Layer

The system uses a **Unified AI Orchestrator** (`backend/app/core/ai_orchestrator.py`) that combines all frameworks into cohesive workflows:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Unified AI Orchestrator                       â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   LangChain   â”‚  â”‚  Pydantic AI â”‚  â”‚   Mem0   â”‚  â”‚ LlamaI â”‚ â”‚
â”‚  â”‚   Agents &    â”‚  â”‚   Type-Safe  â”‚  â”‚ Persistentâ”‚  â”‚  RAG   â”‚ â”‚
â”‚  â”‚   Tools       â”‚  â”‚   Outputs    â”‚  â”‚  Memory  â”‚  â”‚ System â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                  â”‚                 â”‚            â”‚      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                 â”‚            â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  REST API Endpoints  â”‚
                         â”‚  /api/ai-advanced/*  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   React Frontend    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow Example: Intelligent Lead Scoring

1. **Memory Recall** (Mem0): Retrieve past interactions with lead
2. **RAG Search** (LlamaIndex): Find similar high-value leads
3. **Type-Safe Scoring** (Pydantic AI): Generate validated LeadScore object
4. **Memory Storage** (Mem0): Remember scoring decision for future use

---

## ðŸš€ Installation & Setup

### 1. Install Dependencies

All dependencies are already added to `requirements.txt`:

```bash
cd backend
pip install -r requirements.txt
```

**New dependencies installed**:
- `langchain>=0.3.0`
- `langchain-openai>=0.2.0`
- `langchain-anthropic>=0.2.0`
- `langchain-community>=0.3.0`
- `pydantic-ai>=0.0.14`
- `pydantic-ai-slim>=0.0.14`
- `mem0ai>=0.1.0`
- `llama-index>=0.12.0`
- `llama-index-core>=0.12.0`
- `llama-index-llms-openai>=0.3.0`
- `llama-index-embeddings-openai>=0.3.0`
- `reflex>=0.6.0` (optional)
- `reflex-chakra>=0.6.0` (optional)

### 2. Environment Variables

Add to `backend/.env`:

```env
# AI Provider (openai or anthropic)
AI_PROVIDER=openai
AI_MODEL=gpt-4
AI_TEMPERATURE=0.7

# OpenAI
OPENAI_API_KEY=your_openai_key

# Anthropic (optional)
ANTHROPIC_API_KEY=your_anthropic_key

# Mem0 (choose hosted or self-hosted)
MEM0_HOSTED=false  # Set to true for hosted service
MEM0_API_KEY=your_mem0_key  # Required if hosted=true

# Qdrant (for self-hosted Mem0)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Redis (already configured)
REDIS_HOST=localhost
REDIS_PORT=6379
```

### 3. Start External Services

#### Option A: Self-Hosted (Recommended for Development)

**Qdrant (Vector Database for Mem0)**:
```bash
docker run -p 6333:6333 qdrant/qdrant:latest
```

**Redis (Already Running)**:
```bash
# Should already be running for cache/Celery
docker run -p 6379:6379 redis:alpine
```

#### Option B: Hosted Services

- **Mem0**: Sign up at [mem0.ai](https://mem0.ai) and use `MEM0_HOSTED=true`
- **Qdrant Cloud**: Sign up at [cloud.qdrant.io](https://cloud.qdrant.io)

### 4. Start Backend

```bash
cd backend
python3 -m uvicorn app.main:app --reload --port 8000 --host 0.0.0.0
```

The new routes will be available at `/api/ai-advanced/*`

---

## ðŸ“¡ API Endpoints

### Lead Intelligence

#### `POST /api/ai-advanced/lead/score`
Score a lead using intelligent context from memory and similar leads.

**Request**:
```json
{
  "lead_data": {
    "id": 1,
    "name": "John Smith",
    "company": "TechCorp",
    "industry": "SaaS",
    "title": "VP of Sales",
    "email": "john@techcorp.com",
    "engagement": "high"
  }
}
```

**Response**:
```json
{
  "lead_score": {
    "score": 85,
    "confidence": 0.92,
    "factors": [
      "High engagement signals",
      "Decision-making authority",
      "Ideal customer profile match"
    ],
    "recommendation": "Priority outreach within 24 hours"
  },
  "past_interactions": 3,
  "similar_leads_found": 5,
  "context_used": true
}
```

**Features**:
- Recalls past interactions (Mem0)
- Finds similar high-value leads (LlamaIndex)
- Type-safe scoring (Pydantic AI)
- Remembers decision (Mem0)

---

#### `POST /api/ai-advanced/lead/batch-analyze`
Analyze multiple leads in batch with intelligent prioritization.

**Request**:
```json
{
  "leads": [
    {"id": 1, "name": "John", "company": "TechCorp", ...},
    {"id": 2, "name": "Jane", "company": "DataCo", ...}
  ]
}
```

**Response**:
```json
{
  "success": true,
  "analyzed_count": 2,
  "results": [
    {
      "lead": {"id": 2, ...},
      "score": 92,
      "recommendation": "Urgent priority",
      "confidence": 0.95
    },
    {
      "lead": {"id": 1, ...},
      "score": 85,
      "recommendation": "High priority",
      "confidence": 0.92
    }
  ]
}
```

---

### Email Generation

#### `POST /api/ai-advanced/email/generate`
Generate highly personalized email using all available context.

**Request**:
```json
{
  "lead_data": {
    "name": "John Smith",
    "company": "TechCorp",
    "industry": "SaaS",
    "title": "VP of Sales"
  },
  "campaign_objective": "Book demo meeting",
  "tone": "professional"
}
```

**Response**:
```json
{
  "email": {
    "subject": "TechCorp's Q1 Sales Goals - Quick Question",
    "body": "Hi John,\n\nI noticed TechCorp recently expanded to the European market...",
    "tone": "professional",
    "personalization_score": 0.89,
    "call_to_action": "15-minute demo call"
  },
  "preferences_applied": true,
  "past_interactions_considered": 3,
  "campaign_insights_used": 5
}
```

**Features**:
- Applies user email preferences (Mem0)
- Considers past lead interactions (Mem0)
- Uses successful campaign patterns (LlamaIndex)
- Type-safe output (Pydantic AI)
- Remembers generated emails (Mem0)

---

### Campaign Strategy

#### `POST /api/ai-advanced/campaign/strategy`
Create data-driven campaign strategy with tactical recommendations.

**Request**:
```json
{
  "objective": "Generate 100 qualified leads",
  "target_audience": "SaaS companies 100-1000 employees",
  "budget_range": "$10,000-$20,000"
}
```

**Response**:
```json
{
  "strategy": {
    "objective": "Generate qualified leads",
    "target_segments": ["Mid-market SaaS", "Growth-stage startups"],
    "channels": ["Email", "LinkedIn", "Calls"],
    "sequence_steps": 7,
    "estimated_roi": 245.0,
    "timeline_days": 45,
    "budget_estimate": 15000.0
  },
  "tactical_recommendations": "1. Start with personalized LinkedIn...\n2. Follow with email sequence...\n3. Implement A/B testing...",
  "past_learnings_incorporated": 5,
  "historical_data_analyzed": 12
}
```

**Features**:
- Incorporates past campaign learnings (Mem0)
- Analyzes historical performance data (LlamaIndex)
- Type-safe strategy output (Pydantic AI)
- Generates tactical recommendations (LangChain)
- Remembers strategy (Mem0)

---

### Conversational Assistance

#### `POST /api/ai-advanced/conversation`
Get conversational assistance with full context awareness.

**Request**:
```json
{
  "message": "What are the best practices for following up with leads who haven't responded?",
  "context": "Email campaign follow-up"
}
```

**Response**:
```json
{
  "response": "Based on your past campaigns and industry best practices:\n\n1. Wait 3-5 business days before first follow-up\n2. Change the subject line and approach\n3. Add value with relevant resources\n4. Consider switching channels (email â†’ LinkedIn)\n5. Your highest-converting follow-up timing is Tuesday 10am",
  "memory_context_used": true,
  "knowledge_base_used": true,
  "intermediate_steps": [
    {"tool": "search_memory", "result": "Found 5 relevant past campaigns"},
    {"tool": "query_knowledge_base", "result": "3 articles on follow-up"}
  ]
}
```

**Features**:
- Retrieves relevant memories (Mem0)
- Searches knowledge base (LlamaIndex)
- Executes agent tools (LangChain)
- Remembers conversation (Mem0)

---

### Memory Management

#### `POST /api/ai-advanced/memory/add`
Add a memory to the user's memory store.

**Request**:
```json
{
  "content": "User prefers casual tone for tech startups, professional for enterprise",
  "category": "user_preference",
  "metadata": {
    "preference_type": "email_tone",
    "importance": "high"
  }
}
```

---

#### `POST /api/ai-advanced/memory/search`
Search user's memories with semantic similarity.

**Request**:
```json
{
  "query": "email tone preferences",
  "category": "user_preference",
  "limit": 10
}
```

**Response**:
```json
{
  "success": true,
  "count": 3,
  "results": [
    {
      "memory": "User prefers casual tone for tech startups",
      "score": 0.95,
      "metadata": {"category": "user_preference"}
    }
  ]
}
```

---

#### `GET /api/ai-advanced/memory/all`
Get all memories for the user.

---

#### `DELETE /api/ai-advanced/memory/{memory_id}`
Delete a specific memory.

---

### RAG (Document Search)

#### `POST /api/ai-advanced/rag/ingest`
Ingest documents into RAG system.

**Request**:
```json
{
  "documents": [
    {
      "content": "Best practices for SaaS sales: Focus on value, not features...",
      "title": "SaaS Sales Guide",
      "category": "sales_playbook"
    }
  ],
  "index_name": "knowledge_base"
}
```

---

#### `POST /api/ai-advanced/rag/query`
Query RAG system for relevant documents.

**Request**:
```json
{
  "query": "How to handle pricing objections in SaaS sales?",
  "index_name": "knowledge_base",
  "top_k": 5
}
```

**Response**:
```json
{
  "success": true,
  "result": {
    "answer": "When handling pricing objections in SaaS sales: 1. Understand the root concern...",
    "sources": [
      {
        "text": "Pricing objections often stem from...",
        "score": 0.92,
        "metadata": {"category": "sales_playbook"}
      }
    ]
  }
}
```

---

#### `POST /api/ai-advanced/rag/chat`
Chat with documents using RAG.

**Request**:
```json
{
  "query": "Tell me more about value-based selling",
  "index_name": "knowledge_base"
}
```

---

### System Status

#### `GET /api/ai-advanced/status`
Get status of all AI systems.

**Response**:
```json
{
  "success": true,
  "status": {
    "langchain": {"initialized": true},
    "pydantic_ai": {"initialized": true},
    "mem0": {"initialized": true},
    "llamaindex": {"initialized": true},
    "model_provider": "openai",
    "model_name": "gpt-4"
  }
}
```

---

## ðŸ”§ Implementation Details

### File Structure

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ integrations/          # NEW: Integration modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ langchain_agent.py       # LangChain orchestration
â”‚   â”‚   â”œâ”€â”€ pydantic_agent.py        # Pydantic AI type-safe agents
â”‚   â”‚   â”œâ”€â”€ mem0_memory.py           # Mem0 memory layer
â”‚   â”‚   â””â”€â”€ llamaindex_rag.py        # LlamaIndex RAG system
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ ai_orchestrator.py       # NEW: Unified orchestrator
â”‚   â”‚
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â””â”€â”€ ai_advanced.py       # NEW: Advanced AI endpoints
â”‚   â”‚
â”‚   â””â”€â”€ main.py                      # UPDATED: Register ai_advanced_router
â”‚
â”œâ”€â”€ requirements.txt                 # UPDATED: Added 13 new dependencies
â””â”€â”€ .env                            # UPDATED: Add AI config vars
```

### Key Classes

#### 1. `LangChainOrchestrator`
```python
orchestrator = LangChainOrchestrator(
    model_provider="openai",
    model_name="gpt-4",
    temperature=0.7,
)

result = await orchestrator.execute("Research TechCorp and suggest outreach strategy")
```

**Custom Tools**:
- `research_lead`: Gather lead intelligence
- `analyze_campaign`: Campaign performance analysis
- `generate_email`: Email content generation
- `schedule_followup`: Task scheduling

---

#### 2. `PydanticAIAgent`
```python
agent = PydanticAIAgent(
    model_provider="openai",
    model_name="gpt-4",
)

# Type-safe lead scoring
lead_score: LeadScore = await agent.score_lead(lead_data, context)
# lead_score.score is int (0-100)
# lead_score.confidence is float (0.0-1.0)
# Fully validated by Pydantic
```

**Output Models**:
- `LeadScore`: Lead scoring with validation
- `EmailGeneration`: Email with subject/body validation
- `CampaignStrategy`: Strategy with type-safe fields
- `SentimentAnalysis`: Email response sentiment

---

#### 3. `Mem0MemoryManager`
```python
memory = Mem0MemoryManager(hosted=False)

# Remember lead interaction
await memory.remember_lead_interaction(
    user_id="user_123",
    lead_name="John Smith",
    lead_company="TechCorp",
    interaction_type="email",
    interaction_content="Discussed pricing...",
    sentiment="positive",
)

# Recall lead history
history = await memory.recall_lead_history(
    user_id="user_123",
    lead_name="John Smith",
    limit=10,
)
```

**Helper Methods**:
- `remember_lead_interaction()`: Log interactions
- `remember_campaign_insight()`: Store learnings
- `remember_user_preference()`: Save preferences
- `recall_lead_history()`: Get interaction history
- `recall_campaign_learnings()`: Past campaign insights
- `get_context_for_conversation()`: Relevant context

---

#### 4. `LlamaIndexRAG`
```python
rag = LlamaIndexRAG(
    llm_model="gpt-4",
    embedding_model="text-embedding-3-small",
)

# Ingest lead database
await rag.ingest_lead_database(leads, index_name="leads")

# Find similar leads
similar = await rag.find_similar_leads(
    lead_description="SaaS company CTO interested in API integration",
    top_k=5,
    min_score=80,
)

# Chat with documents
response = await rag.chat(
    message="What leads should I prioritize?",
    index_name="leads",
)
```

**Helper Methods**:
- `ingest_lead_database()`: Index lead data
- `ingest_campaign_results()`: Index campaigns
- `ingest_knowledge_base()`: Index articles
- `find_similar_leads()`: Semantic lead search
- `analyze_campaign_patterns()`: Pattern analysis
- `search_knowledge_base()`: KB search

---

#### 5. `UnifiedAIOrchestrator`
```python
orchestrator = UnifiedAIOrchestrator(
    model_provider="openai",
    model_name="gpt-4",
)

# High-level workflows
score_result = await orchestrator.intelligent_lead_scoring(user_id, lead_data)
email_result = await orchestrator.personalized_email_generation(user_id, lead_data, objective, tone)
strategy_result = await orchestrator.strategic_campaign_planning(user_id, objective, audience, budget)
conversation_result = await orchestrator.conversational_assistance(user_id, message)
```

**Orchestrated Workflows**:
- `intelligent_lead_scoring()`: Memory + RAG + Type-safe scoring
- `personalized_email_generation()`: Preferences + Interactions + Insights
- `strategic_campaign_planning()`: Learnings + Historical data + Recommendations
- `conversational_assistance()`: Memory + KB + Agent tools
- `batch_lead_analysis()`: Batch scoring with prioritization

---

## ðŸŽ¨ Frontend Integration

### Example: Lead Scoring Component

```jsx
import { useEffect, useState } from 'react';
import { dataService } from '../lib/dataService';

export function LeadScorer({ lead }) {
  const [score, setScore] = useState(null);
  const [loading, setLoading] = useState(false);

  const scoreLead = async () => {
    setLoading(true);
    try {
      const result = await dataService.post('/ai-advanced/lead/score', {
        lead_data: lead
      });
      setScore(result.lead_score);
    } catch (error) {
      console.error('Scoring failed:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="lead-scorer">
      <button onClick={scoreLead} disabled={loading}>
        {loading ? 'Scoring...' : 'Score Lead'}
      </button>
      
      {score && (
        <div className="score-result">
          <div className="score-badge">
            {score.score}/100
          </div>
          <div className="confidence">
            Confidence: {(score.confidence * 100).toFixed(0)}%
          </div>
          <div className="factors">
            <h4>Factors:</h4>
            <ul>
              {score.factors.map((factor, i) => (
                <li key={i}>{factor}</li>
              ))}
            </ul>
          </div>
          <div className="recommendation">
            <strong>Recommendation:</strong> {score.recommendation}
          </div>
        </div>
      )}
    </div>
  );
}
```

### Example: AI Assistant Chat

```jsx
import { useState } from 'react';
import { dataService } from '../lib/dataService';

export function AIAssistantChat() {
  const [message, setMessage] = useState('');
  const [conversation, setConversation] = useState([]);
  const [loading, setLoading] = useState(false);

  const sendMessage = async () => {
    if (!message.trim()) return;

    const userMessage = { role: 'user', content: message };
    setConversation(prev => [...prev, userMessage]);
    setMessage('');
    setLoading(true);

    try {
      const result = await dataService.post('/ai-advanced/conversation', {
        message: message
      });

      const assistantMessage = { 
        role: 'assistant', 
        content: result.response,
        context_used: result.memory_context_used
      };
      setConversation(prev => [...prev, assistantMessage]);
    } catch (error) {
      console.error('Chat failed:', error);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="ai-chat">
      <div className="messages">
        {conversation.map((msg, i) => (
          <div key={i} className={`message ${msg.role}`}>
            <div className="content">{msg.content}</div>
            {msg.context_used && (
              <div className="badge">Used past context</div>
            )}
          </div>
        ))}
      </div>

      <div className="input-area">
        <input
          value={message}
          onChange={e => setMessage(e.target.value)}
          onKeyPress={e => e.key === 'Enter' && sendMessage()}
          placeholder="Ask Ava anything..."
          disabled={loading}
        />
        <button onClick={sendMessage} disabled={loading}>
          Send
        </button>
      </div>
    </div>
  );
}
```

---

## ðŸ§ª Testing

### Unit Tests

```python
# backend/tests/test_ai_advanced.py
import pytest
from app.core.ai_orchestrator import UnifiedAIOrchestrator

@pytest.mark.asyncio
async def test_lead_scoring():
    orchestrator = UnifiedAIOrchestrator()
    
    lead_data = {
        "name": "John Smith",
        "company": "TechCorp",
        "industry": "SaaS",
        "title": "VP of Sales",
    }
    
    result = await orchestrator.intelligent_lead_scoring(
        user_id="test_user",
        lead_data=lead_data
    )
    
    assert result["lead_score"]["score"] >= 0
    assert result["lead_score"]["score"] <= 100
    assert result["lead_score"]["confidence"] >= 0.0
    assert result["lead_score"]["confidence"] <= 1.0
    assert len(result["lead_score"]["factors"]) > 0
```

### Integration Tests

```python
@pytest.mark.asyncio
async def test_email_generation_api(client):
    response = client.post("/api/ai-advanced/email/generate", json={
        "lead_data": {
            "name": "John Smith",
            "company": "TechCorp",
            "industry": "SaaS"
        },
        "campaign_objective": "Book demo",
        "tone": "professional"
    })
    
    assert response.status_code == 200
    data = response.json()
    assert "email" in data
    assert "subject" in data["email"]
    assert "body" in data["email"]
    assert len(data["email"]["subject"]) >= 5
    assert len(data["email"]["body"]) >= 50
```

---

## ðŸ“Š Performance Considerations

### Caching Strategy

1. **LLM Responses**: Cache identical queries for 1 hour
2. **Lead Scores**: Cache for 24 hours (invalidate on data update)
3. **RAG Embeddings**: Persist in Qdrant/vector store
4. **Memory Context**: Redis-backed with TTL

### Optimization Tips

```python
# Use batch processing for multiple leads
results = await orchestrator.batch_lead_analysis(user_id, leads)

# Pre-load indices during startup
@app.on_event("startup")
async def preload_rag():
    orchestrator = get_orchestrator()
    # Preload common indices
    await orchestrator.rag.ingest_lead_database(all_leads)
```

### Rate Limiting

- **OpenAI**: 10,000 TPM (tokens per minute) on tier 1
- **Anthropic**: 50,000 TPM on tier 1
- Consider implementing queue system for high-volume operations

---

## ðŸ”’ Security & Privacy

### Data Isolation

- Memories are partitioned by `user_id`, `agent_id`, `app_id`
- RAG indices are user-specific (e.g., `leads_user_123`)
- No cross-user data leakage

### PII Handling

```python
from app.core.security import sanitize_text

# Sanitize before storing in memory
sanitized_content = sanitize_text(user_input)
await memory.add_memory(messages=sanitized_content, ...)
```

### API Key Security

- Store API keys in environment variables
- Never commit `.env` to version control
- Rotate keys regularly
- Monitor usage for anomalies

---

## ðŸ“ˆ Monitoring & Observability

### Metrics to Track

```python
# Prometheus metrics
ai_advanced_requests_total = Counter("ai_advanced_requests_total", "Total requests")
ai_advanced_latency = Histogram("ai_advanced_latency_seconds", "Request latency")
ai_advanced_errors = Counter("ai_advanced_errors_total", "Total errors")
```

### Logging

```python
import logging
logger = logging.getLogger(__name__)

logger.info(f"Lead scored: {lead_id}, score: {score}, user: {user_id}")
logger.warning(f"High latency detected: {latency_ms}ms")
logger.error(f"Scoring failed: {error}", exc_info=True)
```

### Sentry Integration

All exceptions are automatically captured by Sentry (already configured in `app/main.py`).

---

## ðŸš¢ Deployment

### Production Checklist

- [ ] Set `AI_PROVIDER` and `AI_MODEL` env vars
- [ ] Configure `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`
- [ ] Deploy Qdrant (vector DB) - use managed cloud or self-hosted
- [ ] Deploy Redis (cache/queue) - already in place
- [ ] Set `MEM0_HOSTED=true` and `MEM0_API_KEY` (or self-host)
- [ ] Configure rate limiting for LLM endpoints
- [ ] Set up monitoring dashboards (Prometheus/Grafana)
- [ ] Enable Sentry error tracking
- [ ] Test all endpoints with realistic data
- [ ] Load test high-volume scenarios (batch analysis)

### Docker Compose

```yaml
services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  qdrant_data:
  redis_data:
```

---

## ðŸŽ“ Training & Adoption

### Developer Onboarding

1. **Read this guide** - Complete integration overview
2. **Run examples** - Execute example usage in each integration file
3. **Test endpoints** - Use Swagger UI at `/docs` to test all APIs
4. **Build sample feature** - Create a simple lead scorer using the APIs
5. **Review code** - Study the orchestrator workflows for patterns

### User Documentation

Create user-facing docs explaining:
- What "context-aware" means (past interactions remembered)
- How to interpret lead scores
- How to use conversational assistance effectively
- Privacy: what data is stored and how

---

## ðŸ”® Future Enhancements

### Phase 2 (Next Quarter)

1. **Multi-Modal Support**: Add image/document analysis for lead enrichment
2. **Voice Integration**: TTS/STT for voice-based lead qualification
3. **Fine-Tuning**: Custom models for industry-specific use cases
4. **Reflex UI Migration** (Optional): Rebuild frontend entirely in Python

### Phase 3 (6 Months)

1. **AutoML Pipeline**: Automated model selection and hyperparameter tuning
2. **Federated Learning**: Cross-organization learnings (privacy-preserving)
3. **Real-Time Streaming**: Process leads/emails in real-time with RAG
4. **Multi-Language**: Support 10+ languages for global sales teams

---

## ðŸ†˜ Troubleshooting

### Common Issues

#### 1. "Qdrant connection refused"

```bash
# Verify Qdrant is running
docker ps | grep qdrant

# Check logs
docker logs <qdrant_container_id>

# Restart Qdrant
docker restart <qdrant_container_id>
```

#### 2. "OpenAI API key not found"

```bash
# Check .env file
cat backend/.env | grep OPENAI

# Set env var manually
export OPENAI_API_KEY=sk-...
```

#### 3. "Memory search returns no results"

```python
# Verify memories exist
memories = await orchestrator.memory.get_all_memories(user_id=user_id)
print(f"Total memories: {len(memories)}")

# Check embedding model is working
await orchestrator.rag.query(query="test", index_name="test")
```

#### 4. "High latency on lead scoring"

- Check network latency to OpenAI/Anthropic APIs
- Enable response caching for repeated queries
- Use batch processing for multiple leads
- Consider using smaller/faster models for real-time use cases

---

## ðŸ“š Additional Resources

### Documentation

- **LangChain**: https://python.langchain.com/docs/
- **Pydantic AI**: https://ai.pydantic.dev/
- **Mem0**: https://docs.mem0.ai/
- **LlamaIndex**: https://docs.llamaindex.ai/
- **Reflex**: https://reflex.dev/docs/

### Community

- **Discord**: Join AI framework communities for support
- **GitHub Issues**: Report bugs/request features in respective repos
- **Stack Overflow**: Tag questions with framework names

### Blog Posts

- [Building Production RAG Systems](https://blog.llamaindex.ai/)
- [Type-Safe AI with Pydantic](https://ai.pydantic.dev/blog/)
- [Memory Management for AI Agents](https://docs.mem0.ai/blog/)

---

## âœ… Summary

This integration brings **enterprise-grade AI capabilities** to the Artisan platform:

âœ… **4 Production-Ready Frameworks** integrated and working together  
âœ… **13 New Dependencies** installed and configured  
âœ… **7 Core Integration Modules** (langchain, pydantic-ai, mem0, llamaindex, orchestrator, routes, docs)  
âœ… **15+ API Endpoints** for advanced AI features  
âœ… **Type-Safe Outputs** with Pydantic validation  
âœ… **Persistent Memory** across sessions with Mem0  
âœ… **RAG System** for document search and chat  
âœ… **Agent Orchestration** with tools and chains  
âœ… **Frontend Examples** ready to implement  
âœ… **Production Deployment** guide included  

**Next Steps**:
1. Install dependencies: `pip install -r requirements.txt`
2. Start external services (Qdrant, Redis)
3. Configure environment variables
4. Test endpoints in Swagger UI (`/docs`)
5. Build frontend components using provided examples

**Questions?** Review the troubleshooting section or check framework-specific docs.

---

**Implementation Complete** âœ…  
**Status**: Production-ready  
**Deployment**: Ready for testing and rollout
