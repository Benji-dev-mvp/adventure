# AI Model Routing Configuration
#
# Externalized model routing so you can flip between models
# per endpoint without code changes
#
# Format:
#   use_case:
#     provider: openai | anthropic
#     model: model-name
#     temperature: 0.0-2.0
#     max_tokens: integer
#     override_policy: true/false  # Whether to override default policy

# Default provider (fallback if not specified per use case)
default_provider: openai
default_model: gpt-4

# Use case-specific routing
use_cases:
  lead_scoring:
    provider: openai
    model: gpt-4o-mini  # Fast and cheap for structured output
    temperature: 0.3
    max_tokens: 500
    
  email_generation:
    provider: openai
    model: gpt-4
    temperature: 0.8
    max_tokens: 800
    
  campaign_strategy:
    provider: openai
    model: gpt-4
    temperature: 0.7
    max_tokens: 2000
    streaming: true
    
  conversation:
    provider: openai
    model: gpt-4
    temperature: 0.7
    max_tokens: 1500
    streaming: true
    
  sentiment_analysis:
    provider: openai
    model: gpt-4o-mini
    temperature: 0.2
    max_tokens: 300

# Feature flags
features:
  enable_streaming: true
  enable_function_calling: true
  enable_memory: true
  enable_rag: true
  enable_caching: true
  enable_tracing: true

# Cost controls (global defaults)
budget:
  max_tokens_per_request: 10000
  max_cost_per_request: 1.0  # USD
  daily_token_limit_per_user: 500000
  daily_cost_limit_per_user: 50.0  # USD
  daily_token_limit_per_org: 5000000
  daily_cost_limit_per_org: 500.0  # USD

# Memory configuration
memory:
  provider: mem0  # mem0, redis, or local
  default_ttl_seconds: 86400  # 24 hours
  max_memory_tokens: 4000
  compression_enabled: true
  redact_pii: true
  
# RAG configuration  
rag:
  provider: llamaindex  # llamaindex or custom
  vector_store: qdrant
  default_similarity_threshold: 0.7
  max_results: 5
  hybrid_search: true
  index_version: kb_v1
  
# Performance
performance:
  cache_ttl_seconds: 300  # 5 minutes default
  request_timeout_seconds: 30
  max_concurrent_requests: 100
  
# Observability
observability:
  structured_logging: true
  tracing_enabled: true
  metrics_enabled: true
  log_token_usage: true
  log_latency: true
  sentry_enabled: true
